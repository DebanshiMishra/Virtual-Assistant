{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paIoBUL9ubdc"
   },
   "source": [
    "# Building a Chatbot using Deep NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQY4qNJXumaH"
   },
   "source": [
    "We have taken dialouges from different movies as our training data, since these are in the form of conversations it'll be helpful in training our network to learn since we building a virtual assistant chatbot.The aim of this chatbot is not just to give answers in \"Yes\" and \"No\", but full length answers corresponding to the question asked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04zs-Ie43plH"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re # used to clean the text and replace some characters by more simple characters\n",
    "import time # check the time of each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-do3Gy54MiS"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFjXsyXe4PwQ"
   },
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIWZhWZi4KxH"
   },
   "outputs": [],
   "source": [
    "lines = open(\"movie_lines.txt\", encoding='utf-8', errors='ignore' ).read().split('\\n')\n",
    "############## to avoid encoding issue, to ignore any error, to read the dataset, split the observations by the lines\n",
    "conversations = open(\"movie_conversations.txt\", encoding='utf-8', errors='ignore' ).read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwVU41M4PR2v"
   },
   "source": [
    "### Creating a dictionary that maps each line and it's ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAfXoLYzKoYO"
   },
   "source": [
    "Basically what we want is a dataset that contains basically 2 column the input and the output. the I/P will be fed to the Neural Network and the O/P will be the target. The easiest way to do this is with a dictionary, because we can keep track of the conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ln4ma9_bOI68"
   },
   "outputs": [],
   "source": [
    "id2line = {} # initialize a dictionary\n",
    "for line in lines:\n",
    "  _line = line.split(\" +++$+++ \")\n",
    "  if len(_line) == 5:\n",
    "    id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eA46n9-t7VN_"
   },
   "outputs": [],
   "source": [
    "id2line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbSh_2q9NX07"
   },
   "source": [
    "### Creating a list of all the conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCSzUnPFM_0z"
   },
   "outputs": [],
   "source": [
    "conversations_ids = []\n",
    "for conversation in conversations[:-1]:\n",
    "   # We wanted to not take the last column since it is empty \n",
    "   _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
    "   conversations_ids.append(_conversation.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eowsMl-JQVDD"
   },
   "outputs": [],
   "source": [
    "conversations_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz_2m1AP7vk1"
   },
   "source": [
    "### Getting separately the questions and answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npCi5fPx8brz"
   },
   "source": [
    "We want two separate lists. One for the questions and one for the answers, but of the same size. This is because, for each index \"i\" of these lists, the answer of index \"i\" should be the answer to the question at index \"i\". Therefore it should be well aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xu04s6OI7cCh"
   },
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "for conversation in conversations_ids:\n",
    "  for i in range(len(conversation)-1):\n",
    "    questions.append(id2line[conversation[i]])\n",
    "    answers.append(id2line[conversation[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6U-Yz4Po_ati"
   },
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ed1HgYZX_ZL8"
   },
   "source": [
    "### Doing a first cleaning of the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMBWl7J3_sKK"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  text = text.lower()\n",
    "  text = re.sub(r\"i'm\",\"i am\", text)\n",
    "  text = re.sub(r\"he's\",\"he is\", text)\n",
    "  text = re.sub(r\"that's\",\"that is\", text)\n",
    "  text = re.sub(r\"what's\",\"what is\", text)\n",
    "  text = re.sub(r\"where's\",\"where is\", text)\n",
    "  text = re.sub(r\"\\'ll\",\" will\", text)\n",
    "  text = re.sub(r\"\\'ve\",\" have\", text)\n",
    "  text = re.sub(r\"\\'re\",\" are\", text)\n",
    "  text = re.sub(r\"\\'d\",\" would\", text)\n",
    "  text = re.sub(r\"it's\",\"it is\", text)\n",
    "  text = re.sub(r\"won't\",\"will not\", text)\n",
    "  text = re.sub(r\"can't\",\"cannot\", text)\n",
    "  text = re.sub(r\"[\\-\\(\\)\\\\\\\"\\#\\/\\@\\;\\:\\<\\>\\{\\}\\+\\=\\-\\|\\.\\?\\,\\!]\",\"\", text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7k__c-eAEDXR"
   },
   "source": [
    "### Cleaning the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFB4gw2-EFy9"
   },
   "outputs": [],
   "source": [
    "clean_questions = []\n",
    "for question in questions:\n",
    "  clean_questions.append(clean_text(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPjkP0NKE99c"
   },
   "outputs": [],
   "source": [
    "clean_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sktKjHapEGwu"
   },
   "source": [
    "### Cleaning the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_5Hs_iBEJOk"
   },
   "outputs": [],
   "source": [
    "clean_answers = []\n",
    "for answer in answers:\n",
    "  clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IKEjcdCFsG1"
   },
   "outputs": [],
   "source": [
    "clean_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zjz4xYYRM1nb"
   },
   "source": [
    "### Creating a dictionary that maps each word to its number of occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTmga3SxFtQ3"
   },
   "outputs": [],
   "source": [
    "word2count = {}\n",
    "for question in clean_questions:\n",
    "  for word in question.split():\n",
    "    if word not in word2count:\n",
    "      word2count[word] = 1\n",
    "    else:\n",
    "      word2count[word] += 1\n",
    "for answer in clean_answers:\n",
    "  for word in answer.split():\n",
    "    if word not in word2count:\n",
    "      word2count[word] = 1\n",
    "    else:\n",
    "      word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0X_kuIOrOk4Q"
   },
   "outputs": [],
   "source": [
    "word2count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4XTSqGZOjmL"
   },
   "source": [
    "### Tokenization and Filtering the non frequent words:\n",
    "#### Creating two dictionaries that map the questions words and the answer words to a unique integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODkRY727PT9y"
   },
   "outputs": [],
   "source": [
    "threshold = 20\n",
    "questionswords2int = {}\n",
    "answerswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "  if count >= threshold:\n",
    "    questionswords2int[word] = word_number\n",
    "    word_number += 1 \n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "  if count >= threshold:\n",
    "    answerswords2int[word] = word_number  \n",
    "    word_number += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YF7yHUtnW5c5"
   },
   "outputs": [],
   "source": [
    "questionswords2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUs-9xp2W7aT"
   },
   "outputs": [],
   "source": [
    "answerswords2int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMiLMQ28ie1F"
   },
   "source": [
    "### Adding the last tokens to these two dictionaries \n",
    "#### The start of string i.e. SOS and end of string i.e., EOS. These are useful for the encoder and the decoder in the Seq2Seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8iM3zu0ibnU"
   },
   "outputs": [],
   "source": [
    "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
    "for token in tokens:\n",
    "  questionswords2int[token] = len(questionswords2int) +1\n",
    "for token in tokens:\n",
    "  answerswords2int[token] = len(answerswords2int) +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6kP7_rgm6io"
   },
   "source": [
    "### Creating the inverse dictionary of the answerswords2int dictionary\n",
    "#### We need to do this because we will need the inverse mapping from the integers to the answers words in the implemnetation of the Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eonKpu51XGRJ"
   },
   "outputs": [],
   "source": [
    "answersint2word = {w_i: w for w, w_i in answerswords2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AXYymBko_3E"
   },
   "outputs": [],
   "source": [
    "answersint2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WgnbjxDpwjd"
   },
   "source": [
    "### Adding the End of String token to the end of every answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-NBhC7XpCl1"
   },
   "outputs": [],
   "source": [
    " for i in range(len(clean_answers)):\n",
    "   clean_answers[i] += \" <EOS>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaPLxxY0qypb"
   },
   "outputs": [],
   "source": [
    "clean_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFWs5vYjq788"
   },
   "source": [
    "### Translating all the questions and the answers into integers and replacing all the words that were filtered out by <OUT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlRAxxfVqz5a"
   },
   "outputs": [],
   "source": [
    "questions_into_int = []\n",
    "for question in clean_questions:\n",
    "  ints = []\n",
    "  for word in question.split():\n",
    "    if word not in questionswords2int:\n",
    "      ints.append(questionswords2int['<OUT>'])\n",
    "    else:\n",
    "      ints.append(questionswords2int[word])\n",
    "  questions_into_int.append(ints)\n",
    "answers_into_int = []\n",
    "for answer in clean_answers:\n",
    "  ints = []\n",
    "  for word in answer.split():\n",
    "    if word not in answerswords2int:\n",
    "      ints.append(answerswords2int['<OUT>'])\n",
    "    else:\n",
    "      ints.append(answerswords2int[word])\n",
    "  answers_into_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WluN13v6cDw"
   },
   "outputs": [],
   "source": [
    "questions_into_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7WNFsvnKTCP"
   },
   "source": [
    "### Sorting questions and answers by the length of questions\n",
    "#### This will speed up the training and optimize it (reduce padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Y5ULQpu6j6a"
   },
   "outputs": [],
   "source": [
    "sorted_clean_questions = []\n",
    "sorted_clean_answers = []\n",
    "for length in range(1,30):\n",
    "  for i in enumerate(questions_into_int):\n",
    "    if len(i[1]) == length:\n",
    "      sorted_clean_questions.append(questions_into_int[i[0]])\n",
    "      sorted_clean_answers.append(answers_into_int[i[0]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQQGuF7LNR29"
   },
   "outputs": [],
   "source": [
    "sorted_clean_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAOH1-iyObk8"
   },
   "source": [
    "# Building the SEQ2SEQ Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFhVI2JhOhkR"
   },
   "source": [
    "### Creating placeholders for the inputs and the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNgQNAycNTht"
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "  inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
    "  targets = tf.placeholder(tf.int32, [None, None], name= 'target')\n",
    "  lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "  keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "  return inputs, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcOvefryTRn3"
   },
   "source": [
    "### Preprocessing the Targets\n",
    "#### We need to do this because the decoder will only accept a certain format of the targets. This format is two-fold. First, the target must be in batches. Secondly, each of the answers in the batch of target must start with the <SOS> token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnqW-9x4TYQ8"
   },
   "outputs": [],
   "source": [
    "def preprocess_targets(targets, word2int, batch_size):\n",
    "  left_side = tf.fill([batch_size, 1], word2int[\"<SOS>\"])\n",
    "  right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
    "  preprocessed_targets = tf.concat([left_side, right_side], axis = 1)\n",
    "  return preprocessed_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7P6gEBflcs3z"
   },
   "source": [
    "### Creating the Encoder RNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrZWam9QcbaC"
   },
   "outputs": [],
   "source": [
    "# rnn_inputs: model inputs for the rnn\n",
    "# rnn_size: no. of input tensors of the encoder rnn layer\n",
    "# keep_prob: we need this to apply dropout regularisation to our lstm\n",
    "# sequence_length: the list of the length of each question in batch \n",
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "  # create lstm\n",
    "  lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "  # apply dropout\n",
    "  lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "  encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "  _,encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell, \n",
    "                                                    cell_bw = encoder_cell,\n",
    "                                                    sequence_length = sequence_length,\n",
    "                                                    inputs = rnn_inputs,\n",
    "                                                    dtype = tf.float32)\n",
    "  return encoder_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHHhiDvmiduF"
   },
   "source": [
    "### Decoding the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2fnyj76ht-m"
   },
   "outputs": [],
   "source": [
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, \n",
    "                        sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "  attention_states = tf.zeroes([batch_size, 1, decoder_cell.output_size])\n",
    "  attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "  # attention_keys: the key to be compared with the target states\n",
    "  # attention_values: the values we'll use to construct the context vectors\n",
    "  # attention_score_function: used to compute the similarity between the keys and the target state\n",
    "  # attention_construct_function: used to build the attention_state\n",
    "  training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0], \n",
    "                                                                            attention_keys, \n",
    "                                                                            attention_values, \n",
    "                                                                            attention_score_function, \n",
    "                                                                            attention_construct_function,\n",
    "                                                                            name = 'attn_dec_train')\n",
    "  decoder_output, decoder_final state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                training_decoder_function,\n",
    "                                                                decoder_embedded_input,\n",
    "                                                                sequence_length,\n",
    "                                                                scope = decoding_scope)\n",
    "  decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
    "  return output_function(decoder_output_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxRvmAneqEuV"
   },
   "source": [
    "### Decoding the test/validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-2q9RPCqOE6"
   },
   "outputs": [],
   "source": [
    "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix,\n",
    "                    sos_id, eos_id, maximum_length, num_words, \n",
    "                    sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "  attention_states = tf.zeroes([batch_size, 1, decoder_cell.output_size])\n",
    "  attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "  test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function, \n",
    "                                                                            encoder_state[0], \n",
    "                                                                            attention_keys, \n",
    "                                                                            attention_values, \n",
    "                                                                            attention_score_function, \n",
    "                                                                            attention_construct_function,\n",
    "                                                                            decoder_embeddings_matrix,\n",
    "                                                                            sos_id, eos_id, \n",
    "                                                                            maximum_length, num_words,\n",
    "                                                                            name = 'attn_dec_inf')\n",
    "  text_predictions, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                  test_decoder_function,\n",
    "                                                                  scope = decoding_scope)\n",
    "  return text_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kK9oJJOcst9l"
   },
   "source": [
    "### Creating the Decoder RNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84PILLs1v7wL"
   },
   "outputs": [],
   "source": [
    "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix,\n",
    "                encoder_state, num_words, sequence_length, rnn_size, \n",
    "                num_layers, word2int, keep_prob, batch_size):\n",
    "  with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "    weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
    "    biases = tf.zeroes_initializer()\n",
    "    output_function = lambda x: tf.contrib.layers.fully_connected(x, num_words, None, \n",
    "                                                                  scope = decoding_scope,\n",
    "                                                                  weights_initializer = weights,\n",
    "                                                                  biases_initializer = biases)\n",
    "    training_predictions = decode_training_set(encoder_state, \n",
    "                                               decoder_cell,\n",
    "                                               decoder_embedded_input, sequence_length, \n",
    "                                               decoding_scope, output_function, \n",
    "                                               keep_prob, batch_size )\n",
    "    decoding_scope.reuse_variables()\n",
    "    test_predictions = decode_test_set(encoder_state, \n",
    "                                       decoder_cell, \n",
    "                                       decoder_embeddings_matrix, \n",
    "                                       word2int['<SOS>'], word2int['<EOS>'] , \n",
    "                                       sequence_length-1, \n",
    "                                       num_words, \n",
    "                                       sequence_length, decoding_scope, \n",
    "                                       output_function, keep_prob, batch_size)\n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlMXfxcc2_zM"
   },
   "source": [
    "### Building the Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kopO1tdN2-js"
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, \n",
    "                  answers_num_words, questions_num_words,\n",
    "                  encoder_embedding_size, decoder_embedding_size,\n",
    "                  rnn_size, num_layers, questionswords2int):\n",
    "  encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs, \n",
    "                                                            answers_num_words +1,\n",
    "                                                            encoder_embedding_size,\n",
    "                                                            initializer = tf.random_uniform_initializer(0,1))\n",
    "  encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "  preprocessed_targets = preprocess_targets(targets, questionsword2int, batch_size)\n",
    "  decoder_embeddings_matrix = tf.Variable(tf.random_normal_initializer([questions_num_words +1, decoder_embedding_size], 0, 1))\n",
    "  decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocess_targets )\n",
    "  training_predictions, test_predictions = decoder_rnn(decoder_embedded_input, \n",
    "                                                       decoder_embeddings_matrix,\n",
    "                                                       encoder_state, \n",
    "                                                       questions_num_words, \n",
    "                                                       sequence_length, rnn_size, \n",
    "                                                       num_layers, \n",
    "                                                       questionswords2int, \n",
    "                                                       keep_prob, batch_size)\n",
    "  return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IHs464m8xgq"
   },
   "source": [
    "# Training the SEQ2SEQ Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_BP6oTY54us"
   },
   "source": [
    "### Setting the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2aa5joN9RLY"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "rnn_size = 1024\n",
    "num_layers = 3\n",
    "encoding_embedding_size = 1024\n",
    "decoding_embedding_size = 1024\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KY2RJEhu6Hsy"
   },
   "source": [
    "### Defining a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTaPJWZU6RFk"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pta0Lbu26WWP"
   },
   "source": [
    "### Loading the model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUEaTvvu6dOE"
   },
   "outputs": [],
   "source": [
    "inputs, targets, lr, keep_prob = model_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfL6KpBB6ekT"
   },
   "source": [
    "### Setting the sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kyn-iZF6jxq"
   },
   "outputs": [],
   "source": [
    "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzNcKAyo6oUp"
   },
   "source": [
    "### Getting the shape of the inputs tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zmyg0ocs61zO"
   },
   "outputs": [],
   "source": [
    "input_shape = tf.shape(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwumoVfC6352"
   },
   "source": [
    "### Getting the shape of the inputs tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuR6_JPA67Rm"
   },
   "outputs": [],
   "source": [
    "input_shape = tf.shape(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keNqsCFe6-pT"
   },
   "source": [
    "### Getting the training and test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qw3vkea7CKS"
   },
   "outputs": [],
   "source": [
    "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
    "                                                       targets,\n",
    "                                                       keep_prob,\n",
    "                                                       batch_size,\n",
    "                                                       sequence_length,\n",
    "                                                       len(answerswords2int),\n",
    "                                                       len(questionswords2int),\n",
    "                                                       encoding_embedding_size,\n",
    "                                                       decoding_embedding_size,\n",
    "                                                       rnn_size,\n",
    "                                                       num_layers,\n",
    "                                                       questionswords2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pk5X5A5R7fGc"
   },
   "source": [
    "### Setting up the Loss Error, the Optimizer and Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zz-Zkxiu5DcT"
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"optimization\"):\n",
    "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n",
    "                                                  targets,\n",
    "                                                  tf.ones([input_shape[0], sequence_length]))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(loss_error)\n",
    "    clipped_gradients = [(tf.clip_by_value(grad_tensor, -5., 5.), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None]\n",
    "    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EQwkcZf7k1I"
   },
   "source": [
    "### Padding the sequences with the <PAD> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apssr6ch5SP1"
   },
   "outputs": [],
   "source": [
    "def apply_padding(batch_of_sequences, word2int):\n",
    "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
    "    return [sequence + [word2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bi6kRZQt7wWs"
   },
   "source": [
    "### Splitting the data into batches of questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ea3-HB7z7xdi"
   },
   "outputs": [],
   "source": [
    "def split_into_batches(questions, answers, batch_size):\n",
    "    for batch_index in range(0, len(questions) // batch_size):\n",
    "        start_index = batch_index * batch_size\n",
    "        questions_in_batch = questions[start_index : start_index + batch_size]\n",
    "        answers_in_batch = answers[start_index : start_index + batch_size]\n",
    "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
    "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
    "        yield padded_questions_in_batch, padded_answers_in_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTMkHz7x727x"
   },
   "source": [
    "### Splitting the questions and answers into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXTVFXs174c1"
   },
   "outputs": [],
   "source": [
    "training_validation_split = int(len(sorted_clean_questions) * 0.15)\n",
    "training_questions = sorted_clean_questions[training_validation_split:]\n",
    "training_answers = sorted_clean_answers[training_validation_split:]\n",
    "validation_questions = sorted_clean_questions[:training_validation_split]\n",
    "validation_answers = sorted_clean_answers[:training_validation_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh8_6sd279Yy"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmcCPhwN7-dH"
   },
   "outputs": [],
   "source": [
    "batch_index_check_training_loss = 100\n",
    "batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\n",
    "total_training_loss_error = 0\n",
    "list_validation_loss_error = []\n",
    "early_stopping_check = 0\n",
    "early_stopping_stop = 100\n",
    "checkpoint = \"chatbot_weights.ckpt\"\n",
    "session.run(tf.global_variables_initializer())\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
    "        starting_time = time.time()\n",
    "        _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\n",
    "                                                                                               targets: padded_answers_in_batch,\n",
    "                                                                                               lr: learning_rate,\n",
    "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                                               keep_prob: keep_probability})\n",
    "        total_training_loss_error += batch_training_loss_error\n",
    "        ending_time = time.time()\n",
    "        batch_time = ending_time - starting_time\n",
    "        if batch_index % batch_index_check_training_loss == 0:\n",
    "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
    "                                                                                                                                       epochs,\n",
    "                                                                                                                                       batch_index,\n",
    "                                                                                                                                       len(training_questions) // batch_size,\n",
    "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
    "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
    "            total_training_loss_error = 0\n",
    "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
    "            total_validation_loss_error = 0\n",
    "            starting_time = time.time()\n",
    "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n",
    "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
    "                                                                       targets: padded_answers_in_batch,\n",
    "                                                                       lr: learning_rate,\n",
    "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                       keep_prob: 1})\n",
    "                total_validation_loss_error += batch_validation_loss_error\n",
    "            ending_time = time.time()\n",
    "            batch_time = ending_time - starting_time\n",
    "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
    "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            list_validation_loss_error.append(average_validation_loss_error)\n",
    "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
    "                print('I speak better now!!')\n",
    "                early_stopping_check = 0\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session, checkpoint)\n",
    "            else:\n",
    "                print(\"Sorry I do not speak better, I need to practice more.\")\n",
    "                early_stopping_check += 1\n",
    "                if early_stopping_check == early_stopping_stop:\n",
    "                    break\n",
    "    if early_stopping_check == early_stopping_stop:\n",
    "        print(\"My apologies, I cannot speak better anymore. This is the best I can do.\")\n",
    "        break\n",
    "print(\"Game Over\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7EDs0Ce8NSA"
   },
   "source": [
    "# Testing the SEQ2SEQ Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dSLtGBD8cat"
   },
   "source": [
    "### Loading the weights and Running the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVyiggQw8YF9"
   },
   "outputs": [],
   "source": [
    "checkpoint = \"./chatbot_weights.ckpt\"\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(session, checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXEdw-wx8iX3"
   },
   "source": [
    "### Converting the questions from strings to lists of encoding integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbew_5Zd8jBg"
   },
   "outputs": [],
   "source": [
    "def convert_string2int(question, word2int):\n",
    "    question = clean_text(question)\n",
    "    return [word2int.get(word, word2int['<OUT>']) for word in question.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1vFd2kR8nyl"
   },
   "source": [
    "### Setting up the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCd2BMci8onv"
   },
   "outputs": [],
   "source": [
    "while(True):\n",
    "    question = input(\"You: \")\n",
    "    if question == 'Goodbye':\n",
    "        break\n",
    "    question = convert_string2int(question, questionswords2int)\n",
    "    question = question + [questionswords2int['<PAD>']] * (25 - len(question))\n",
    "    fake_batch = np.zeros((batch_size, 25))\n",
    "    fake_batch[0] = question\n",
    "    predicted_answer = session.run(test_predictions, {inputs: fake_batch, keep_prob: 0.5})[0]\n",
    "    answer = ''\n",
    "    for i in np.argmax(predicted_answer, 1):\n",
    "        if answersints2word[i] == 'i':\n",
    "            token = ' I'\n",
    "        elif answersints2word[i] == '<EOS>':\n",
    "            token = '.'\n",
    "        elif answersints2word[i] == '<OUT>':\n",
    "            token = 'out'\n",
    "        else:\n",
    "            token = ' ' + answersints2word[i]\n",
    "        answer += token\n",
    "        if token == '.':\n",
    "            break\n",
    "    print('ChatBot: ' + answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Chatbot",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
